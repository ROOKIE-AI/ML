{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  activation function (激活函数)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数的推荐\n",
    "\n",
    "激活函数（activation function）是神经网络中的一个关键组件，它决定了神经元的输出。选择适当的激活函数可以显著影响模型的性能和训练效果。下面是一些常用的激活函数及其推荐使用场景：\n",
    "\n",
    "### 1. Sigmoid 函数\n",
    "- **公式**：$ \\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- **优点**：将输出映射到 (0, 1) 之间，适合于处理概率输出的问题。\n",
    "- **缺点**：容易出现梯度消失问题，尤其在深层网络中。\n",
    "- **推荐场景**：适用于二分类问题的输出层。\n",
    "\n",
    "### 2. Tanh 函数\n",
    "- **公式**：$ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "- **优点**：将输出映射到 (-1, 1) 之间，输出值的均值接近 0，收敛速度比 Sigmoid 快。\n",
    "- **缺点**：同样容易出现梯度消失问题。\n",
    "- **推荐场景**：适用于中间层的激活函数，特别是需要零均值输出的场景。\n",
    "\n",
    "### 3. ReLU（Rectified Linear Unit）函数\n",
    "- **公式**：$ \\text{ReLU}(x) = \\max(0, x) $\n",
    "- **优点**：计算简单，不会出现梯度消失问题，促进稀疏激活。\n",
    "- **缺点**：容易出现“神经元死亡”问题，即当 x < 0 时，梯度为 0，导致神经元不再更新。\n",
    "- **推荐场景**：适用于大多数隐藏层，特别是深层神经网络。\n",
    "\n",
    "### 4. Leaky ReLU 函数\n",
    "- **公式**：$ \\text{Leaky ReLU}(x) = \\max(0.01x, x) $\n",
    "- **优点**：解决了 ReLU 的“神经元死亡”问题。\n",
    "- **缺点**：引入了一个新的超参数（负斜率），需要调整。\n",
    "- **推荐场景**：与 ReLU 类似，但在处理负输入时表现更好。\n",
    "\n",
    "### 5. Parametric ReLU (PReLU) 函数\n",
    "- **公式**：$ \\text{PReLU}(x) = \\max(\\alpha x, x) $\n",
    "- **优点**：类似 Leaky ReLU，但斜率参数 α 可学习，适应性更强。\n",
    "- **缺点**：增加了计算复杂度和模型参数量。\n",
    "- **推荐场景**：深层神经网络，需要更强适应性的场景。\n",
    "\n",
    "### 6. Softmax 函数\n",
    "- **公式**：$ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $\n",
    "- **优点**：将输出映射到 (0, 1) 之间，并且输出之和为 1，适合于多分类问题。\n",
    "- **缺点**：对于数值过大或过小时，计算可能不稳定。\n",
    "- **推荐场景**：多分类问题的输出层。\n",
    "\n",
    "### 7. Swish 函数\n",
    "- **公式** $ \\text{swish}(x) = x \\cdot \\sigma(x) $\n",
    "- **优点**：在许多深度学习任务中表现优于 ReLU 和 Sigmoid，具有平滑的非线性。\n",
    "- **缺点**：计算复杂度较高。\n",
    "- **推荐场景**：需要高性能和稳定性的深度神经网络。\n",
    "\n",
    "### 8. GELU（Gaussian Error Linear Unit）函数\n",
    "- **公式**：$ \\text{GELU}(x) = x \\cdot \\Phi(x) $\n",
    "- 其中 $\\Phi(x)$ 是标准正态分布的累积分布函数。\n",
    "- **优点**：在许多任务中表现优于 ReLU 和 Swish，具有良好的平滑性。\n",
    "- **缺点**：计算复杂度较高。\n",
    "- **推荐场景**：最新的深度学习模型，特别是Transformer架构中。\n",
    "\n",
    "### 总结\n",
    "不同的激活函数适用于不同的场景和需求。总体来说，ReLU 及其变种（如 Leaky ReLU 和 PReLU）是目前使用最广泛的激活函数，特别是在深层神经网络中。如果对模型性能要求较高，可以尝试 Swish 或 GELU 等新型激活函数。在输出层，根据具体任务选择合适的激活函数，如 Sigmoid 用于二分类，Softmax 用于多分类。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 为什么softmax激活函数使用自然指数相加的形式计算概率?而不是线性相加呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Softmax函数使用自然指数而不是线性方式来计算权重是因为指数函数具有一些独特的数学性质，这些性质使得softmax函数在许多机器学习和深度学习任务中非常有用。以下是采用自然指数而不是线性方式的几个主要原因：\n",
    "    1. 保证非负性\n",
    "    2. 增强区分度\n",
    "    3. 保持输出为概率分布\n",
    "    4. 数学上的便利性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可视化softmax和线性单元的区分度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baff753010e4698bf5314031de67719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w', max=50.0, min=0.1), IntSlider(value=10, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.exp_line(w, _max)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_line(w, _max):\n",
    "    x = np.arange(_max) * w\n",
    "    y_line = x / x.sum()\n",
    "    y_exp = np.exp(x) / np.exp(x).sum()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y_line, label='line')  \n",
    "    plt.plot(x, y_exp, label='exp')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Linear vs Exponential Distribution')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "interact(exp_line, w=widgets.FloatSlider(value=1, min=0.1, max=50.0, step=0.1, description='w'),\n",
    "         _max=widgets.IntSlider(value=10, min=1, max=100, step=1, description='_max'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
